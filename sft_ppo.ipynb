{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869cf54f",
   "metadata": {},
   "source": [
    "# env\n",
    "\n",
    "trl==0.10.1 \n",
    "\n",
    "transformers==4.45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z1SSU_rOQjjn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z1SSU_rOQjjn",
    "outputId": "00cc17e9-c76f-4a01-ff59-54d79911a88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3c8c67a-9d38-4608-9a1b-551f410c609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hANKKJWERZAG",
   "metadata": {
    "id": "hANKKJWERZAG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \n",
    "os.environ[\"WANDB_API_KEY\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0SVJsxZCajRV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SVJsxZCajRV",
    "outputId": "4658fa25-2427-4d85-e4fc-20b2916752fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting accelerate\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2c/92/48aec3736ca778ffe5fa68e19e3c18917cba4de43fa46fe6176cccafe267/accelerate-1.0.1-py3-none-any.whl (330 kB)\n",
      "\u001b[K     |████████████████████████████████| 330 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ed/a5/33cf000137545a08b0a3a6ea76c8ccbd87917f78bb5d737f9f56f3b11ef6/datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "\u001b[K     |████████████████████████████████| 480 kB 19.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting peft\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/78/9d/5f95bfb298c8d3b4e3a107701f9a4e7774a0d4d1f8eb0c9d5420b80f7c9d/peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[K     |████████████████████████████████| 320 kB 48.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/51/51/b87caa939fedf307496e4dbf412f4b909af3d9ca8b189fc3b65c1faa456f/transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.0 MB 46.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<3.0.0,>=1.17 in ./miniconda3/lib/python3.8/site-packages (from accelerate) (1.23.1)\n",
      "Collecting torch>=1.10.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a9/71/45aac46b75742e08d2d6f9fc2b612223b5e36115b8b2ed673b23c21b5387/torch-2.4.1-cp38-cp38-manylinux1_x86_64.whl (797.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 797.1 MB 4.5 MB/s eta 0:00:0101    |████████████▍                   | 309.9 MB 70.3 MB/s eta 0:00:07\n",
      "\u001b[?25hCollecting pyyaml\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fd/7f/2c3697bba5d4aa5cc2afe81826d73dfae5f049458e44732c7a0938baa673/PyYAML-6.0.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (746 kB)\n",
      "\u001b[K     |████████████████████████████████| 746 kB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting safetensors>=0.4.3\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[K     |████████████████████████████████| 471 kB 81.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub>=0.21.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/59/a8/4677014e771ed1591a87b63a2392ce6923baf807193deef302dcfde17542/huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[K     |████████████████████████████████| 558 kB 56.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: psutil in ./miniconda3/lib/python3.8/site-packages (from accelerate) (5.9.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.8/site-packages (from accelerate) (21.3)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/6d/2f/6cad7b5fe86b7652579346cb7f85156c11761df26435651cbba89376cd2c/hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 106.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting typing-extensions>=3.7.4.3\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/8b/54/b1ae86c0973cc6f0210b53d508ca3641fb6d0c56823f288d108bc7ab3cc8/typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 93.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b9/f8/feced7779d755758a52d1f6635d990b8d98dc0a29fa568bbe0625f18fdf3/filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/56/53/eb690efa8513166adef3e0669afd31e95ffde69fb3c52ec2ac7223ed6018/fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[K     |████████████████████████████████| 193 kB 64.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.61.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging>=20.0->accelerate) (3.0.9)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.1 MB 39.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 121.6 MB 20.6 MB/s eta 0:00:01    |████████████████▋               | 63.3 MB 13.9 MB/s eta 0:00:05\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 410.6 MB 10.1 MB/s eta 0:00:01     |████████████████████▉           | 266.7 MB 4.6 MB/s eta 0:00:32\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 664.8 MB 9.8 MB/s eta 0:00:0113    |███████████                     | 227.3 MB 3.9 MB/s eta 0:01:54     |███████████████████████         | 477.5 MB 5.3 MB/s eta 0:00:36     |████████████████████████▌       | 509.9 MB 10.7 MB/s eta 0:00:15\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 56.5 MB 9.5 MB/s eta 0:00:012     |████████████▎                   | 21.7 MB 24.7 MB/s eta 0:00:02     |██████████████▎                 | 25.1 MB 24.7 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7 MB 25.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting triton==3.0.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/4d/b4/c37e2776a1390bab7e78a6d52bd525441cb3cad7260a6a00b11b0b702e7c/triton-3.0.0-1-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 209.4 MB 4.5 MB/s eta 0:00:012    |████████████                    | 78.2 MB 23.8 MB/s eta 0:00:060.1 MB 2.0 MB/s eta 0:00:26\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 58.8 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/a8/05/9d4f9b78ead6b2661d6e8ea772e111fc4a9fbd866ad0c81906c11206b55e/networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 73.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 124.2 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 196.0 MB 10.1 MB/s eta 0:00:01     |███████████████████████████▎    | 167.0 MB 12.7 MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/4b/2a/0a131f572aa09f741c30ccd45a8e56316e8be8dfc7bc19bf0ab7cfef7b19/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 176.2 MB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[K     |████████████████████████████████| 823 kB 26.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/99/ff/c87e0622b1dadea79d2fb0b25ade9ed98954c9033722eb707053d310d4f3/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2 MB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/46/0c/c75bbfb967457a0b7670b8ad267bfc4fffdf341c074e0a80db06c24ccfd4/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 39.7 MB 2.2 MB/s eta 0:00:012\n",
      "\u001b[?25hCollecting dill<0.3.9,>=0.3.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 89.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/62/f1/0d1bb3518dbcf4424c5d3e0b259d742e50a51753036799e8a6f1714e4fb9/xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 76.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess<0.70.17\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ea/89/38df130f2c799090c978b366cfdf5b96d08de5b29a4a293df7f7429fa50b/multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 67.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/03/be/7ad9a6cd2312221cf7b6837d8e2d8e4660fbd4f9f15bccf79ef857f41f4d/aiohttp-3.10.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 78.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=15.0.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e6/c1/4c6bcdf7a820034aa91a8b4d25fef38809be79b42ca7aaa16d4680b0bbac/pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 40.0 MB 3.3 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/7c/e4/56027c4a6b4ae70ca9de302488c5ca95ad4a39e190093d6c1a8ace08341b/requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 107.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/f8/7f/5b047effafbdd34e52c9e2d7e44f729a0655efafb22198c45cf692cdc157/pandas-2.0.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.4 MB 140.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.42.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[K     |████████████████████████████████| 78 kB 102.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.9.0,>=2023.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/1d/a0/6aaea0c2fbea2f89bfd5db25fb1e3481896a423002ebe4e55288907a97a3/fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "\u001b[K     |████████████████████████████████| 179 kB 130.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2e/a9/83692e37d8152f104333132105b67100aabfb2e96a87f6bed67f566035a7/multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 139.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/66/30/f9c006223feb2ac87f1826b57f2367b60aacc43092f562dab60d2312562e/frozenlist-1.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 152.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.12.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c2/1e/1c78c695a4c7b957b5665e46a89ea35df48511dbed301a05c0a8beed0cc3/yarl-1.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[K     |████████████████████████████████| 319 kB 155.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<6.0,>=4.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/fe/ba/e2081de779ca30d473f21f5b30e0e737c438205440784c7dfc81efc2b029/async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b9/74/fbb6559de3607b3300b9be3cc64e97548d55678e44623db17820dbd20002/aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2021.5.30)\n",
      "Collecting charset_normalizer<4,>=2\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/2e/bb/d76d3d6e340fb0967c43c564101e28a78c9a363ea62f736a68af59ee3683/charset_normalizer-3.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 152.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (1.26.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.10)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c3/f1/69a30ff0928d07f50bdc6f0147fd9a08e80904fd3fdb711785e518de1021/propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[K     |████████████████████████████████| 213 kB 152.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers<0.21,>=0.20\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/02/52/cd7b83b6e0a1fda503ca7184b0162583de6d2f176dda9aa02abf80cb247b/tokenizers-0.20.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 148.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/5a/c8/dc7153ceb5bcc344f5c4f0291ea45925a5f00009afa3849e91561ac2e847/regex-2024.11.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[K     |████████████████████████████████| 785 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[K     |████████████████████████████████| 347 kB 147.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[K     |████████████████████████████████| 536 kB 158.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: typing-extensions, nvidia-nvjitlink-cu12, charset-normalizer, tqdm, requests, pyyaml, propcache, nvidia-cusparse-cu12, nvidia-cublas-cu12, multidict, mpmath, hf-xet, fsspec, frozenlist, filelock, yarl, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, networkx, huggingface-hub, async-timeout, aiosignal, aiohappyeyeballs, tzdata, torch, tokenizers, safetensors, regex, dill, aiohttp, xxhash, transformers, pyarrow, pandas, multiprocess, accelerate, peft, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.61.2\n",
      "    Uninstalling tqdm-4.61.2:\n",
      "      Successfully uninstalled tqdm-4.61.2\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.25.1\n",
      "    Uninstalling requests-2.25.1:\n",
      "      Successfully uninstalled requests-2.25.1\n",
      "Successfully installed accelerate-1.0.1 aiohappyeyeballs-2.4.4 aiohttp-3.10.11 aiosignal-1.3.1 async-timeout-5.0.1 charset-normalizer-3.4.2 datasets-3.1.0 dill-0.3.8 filelock-3.16.1 frozenlist-1.5.0 fsspec-2024.9.0 hf-xet-1.1.5 huggingface-hub-0.34.3 mpmath-1.3.0 multidict-6.1.0 multiprocess-0.70.16 networkx-3.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 pandas-2.0.3 peft-0.13.2 propcache-0.2.0 pyarrow-17.0.0 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 sympy-1.13.3 tokenizers-0.20.3 torch-2.4.1 tqdm-4.67.1 transformers-4.46.3 triton-3.0.0 typing-extensions-4.13.2 tzdata-2025.2 xxhash-3.5.0 yarl-1.15.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting bitsandbytes\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/07/b7/cb5ce4d1a382cf53c19ef06c5fc29e85f5e129b4da6527dd207d90a5b8ad/bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 76.1 MB 12.9 MB/s eta 0:00:01   |█▍                              | 3.2 MB 2.2 MB/s eta 0:00:34\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in ./miniconda3/lib/python3.8/site-packages (from bitsandbytes) (1.23.1)\n",
      "Requirement already satisfied: torch<3,>=2.0 in ./miniconda3/lib/python3.8/site-packages (from bitsandbytes) (2.4.1)\n",
      "Requirement already satisfied: sympy in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (11.0.2.54)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (2.20.5)\n",
      "Requirement already satisfied: networkx in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (12.1.3.1)\n",
      "Requirement already satisfied: triton==3.0.0 in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (3.0.0)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.8/site-packages (from torch<3,>=2.0->bitsandbytes) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./miniconda3/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=2.0->bitsandbytes) (12.9.86)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.8/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/lib/python3.8/site-packages (from sympy->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Installing collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.45.5\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting trl\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e4/dd/d2cf3dbc1013cee71ceef584f5ab69915fc05d209ef1e276f8652058c350/trl-0.11.4-py3-none-any.whl (316 kB)\n",
      "\u001b[K     |████████████████████████████████| 316 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in ./miniconda3/lib/python3.8/site-packages (from trl) (2.4.1)\n",
      "Collecting tyro>=0.5.11\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/36/ef/98b2700c6a262a9d78eaec5b16916a75a63f7c1e642cfce0717c440d2f9b/tyro-0.9.27-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 23.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: accelerate in ./miniconda3/lib/python3.8/site-packages (from trl) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.18.2 in ./miniconda3/lib/python3.8/site-packages (from trl) (1.23.1)\n",
      "Requirement already satisfied: datasets in ./miniconda3/lib/python3.8/site-packages (from trl) (3.1.0)\n",
      "Requirement already satisfied: transformers>=4.40.0 in ./miniconda3/lib/python3.8/site-packages (from trl) (4.46.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2.20.5)\n",
      "Requirement already satisfied: triton==3.0.0 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (1.13.3)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (9.1.0.70)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (4.13.2)\n",
      "Requirement already satisfied: networkx in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl) (2024.9.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./miniconda3/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.9.86)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (21.3)\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./miniconda3/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.34.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./miniconda3/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./miniconda3/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (0.5.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.8/site-packages (from transformers>=4.40.0->trl) (2024.11.6)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.40.0->trl) (1.1.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers>=4.40.0->trl) (3.0.9)\n",
      "Collecting docstring-parser>=0.15\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/55/e2/2537ebcff11c1ee1ff17d8d0b6f4db75873e3b0fb32c2d4a2ee31ecb310a/docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Collecting eval-type-backport>=0.1.3\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/ce/31/55cd413eaccd39125368be33c46de24a1f639f2e12349b0361b4678f3915/eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting shtab>=1.5.6\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/74/03/3271b7bb470fbab4adf5bd30b0d32143909d96f3608d815b447357f47f2b/shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Collecting typeguard>=4.0.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/61/a3/00203767544b597a9e3c57b29a84967b3230f00bdd9aa6a52a73187043b4/typeguard-4.4.0-py3-none-any.whl (35 kB)\n",
      "Collecting rich>=11.1.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/e3/30/3c4d035596d3cf444529e0b2953ad0466f6049528a879d27534700580395/rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 12.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting markdown-it-py>=2.2.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/c7/21/705964c7812476f378728bdf590ca4b771ec72385c533964653c68e86bdc/pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 19.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting mdurl~=0.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in ./miniconda3/lib/python3.8/site-packages (from typeguard>=4.0.0->tyro>=0.5.11->trl) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./miniconda3/lib/python3.8/site-packages (from importlib-metadata>=3.6->typeguard>=4.0.0->tyro>=0.5.11->trl) (3.8.1)\n",
      "Requirement already satisfied: psutil in ./miniconda3/lib/python3.8/site-packages (from accelerate->trl) (5.9.1)\n",
      "Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.8/site-packages (from datasets->trl) (3.10.11)\n",
      "Requirement already satisfied: pandas in ./miniconda3/lib/python3.8/site-packages (from datasets->trl) (2.0.3)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./miniconda3/lib/python3.8/site-packages (from datasets->trl) (0.70.16)\n",
      "Requirement already satisfied: xxhash in ./miniconda3/lib/python3.8/site-packages (from datasets->trl) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./miniconda3/lib/python3.8/site-packages (from datasets->trl) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./miniconda3/lib/python3.8/site-packages (from datasets->trl) (0.3.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl) (2.4.4)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl) (5.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl) (6.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl) (22.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl) (1.15.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers>=4.40.0->trl) (1.26.6)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.8/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets->trl) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets->trl) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/lib/python3.8/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Installing collected packages: mdurl, pygments, markdown-it-py, typeguard, shtab, rich, eval-type-backport, docstring-parser, tyro, trl\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.12.0\n",
      "    Uninstalling Pygments-2.12.0:\n",
      "      Successfully uninstalled Pygments-2.12.0\n",
      "Successfully installed docstring-parser-0.17.0 eval-type-backport-0.2.2 markdown-it-py-3.0.0 mdurl-0.1.2 pygments-2.19.2 rich-14.1.0 shtab-1.7.2 trl-0.11.4 typeguard-4.4.0 tyro-0.9.27\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate datasets peft transformers\n",
    "!pip install -U bitsandbytes\n",
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b0aac87-0304-4665-be68-d617d65a56e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Collecting trl==0.10.1\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/73/25/1385321000454726766eb598f0cc798db6d1e668ed314285aa53e8334bed/trl-0.10.1-py3-none-any.whl (280 kB)\n",
      "\u001b[K     |████████████████████████████████| 280 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==4.45.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/31/6d/09a57274b45a411e6cc62122a032dd4f7cc6fb9e5323d4376ccc1d74bc56/transformers-4.45.0-py3-none-any.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.2 in ./miniconda3/lib/python3.8/site-packages (from trl==0.10.1) (1.23.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in ./miniconda3/lib/python3.8/site-packages (from trl==0.10.1) (0.9.27)\n",
      "Requirement already satisfied: datasets in ./miniconda3/lib/python3.8/site-packages (from trl==0.10.1) (3.1.0)\n",
      "Requirement already satisfied: torch>=1.4.0 in ./miniconda3/lib/python3.8/site-packages (from trl==0.10.1) (2.4.1)\n",
      "Requirement already satisfied: accelerate in ./miniconda3/lib/python3.8/site-packages (from trl==0.10.1) (1.0.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (4.67.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (6.0.2)\n",
      "Requirement already satisfied: requests in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (2.32.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (0.34.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (21.3)\n",
      "Requirement already satisfied: filelock in ./miniconda3/lib/python3.8/site-packages (from transformers==4.45.0) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0) (4.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./miniconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.45.0) (1.1.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./miniconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers==4.45.0) (3.0.9)\n",
      "Requirement already satisfied: networkx in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (3.1)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (12.1.3.1)\n",
      "Requirement already satisfied: jinja2 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (3.1.2)\n",
      "Requirement already satisfied: triton==3.0.0 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (2.20.5)\n",
      "Requirement already satisfied: sympy in ./miniconda3/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.10.1) (1.13.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./miniconda3/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl==0.10.1) (12.9.86)\n",
      "Requirement already satisfied: eval-type-backport>=0.1.3 in ./miniconda3/lib/python3.8/site-packages (from tyro>=0.5.11->trl==0.10.1) (0.2.2)\n",
      "Requirement already satisfied: docstring-parser>=0.15 in ./miniconda3/lib/python3.8/site-packages (from tyro>=0.5.11->trl==0.10.1) (0.17.0)\n",
      "Requirement already satisfied: rich>=11.1.0 in ./miniconda3/lib/python3.8/site-packages (from tyro>=0.5.11->trl==0.10.1) (14.1.0)\n",
      "Requirement already satisfied: typeguard>=4.0.0 in ./miniconda3/lib/python3.8/site-packages (from tyro>=0.5.11->trl==0.10.1) (4.4.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in ./miniconda3/lib/python3.8/site-packages (from tyro>=0.5.11->trl==0.10.1) (1.7.2)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./miniconda3/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.10.1) (2.19.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./miniconda3/lib/python3.8/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.10.1) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./miniconda3/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.10.1) (0.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in ./miniconda3/lib/python3.8/site-packages (from typeguard>=4.0.0->tyro>=0.5.11->trl==0.10.1) (4.12.0)\n",
      "Requirement already satisfied: zipp>=0.5 in ./miniconda3/lib/python3.8/site-packages (from importlib-metadata>=3.6->typeguard>=4.0.0->tyro>=0.5.11->trl==0.10.1) (3.8.1)\n",
      "Requirement already satisfied: psutil in ./miniconda3/lib/python3.8/site-packages (from accelerate->trl==0.10.1) (5.9.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in ./miniconda3/lib/python3.8/site-packages (from datasets->trl==0.10.1) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./miniconda3/lib/python3.8/site-packages (from datasets->trl==0.10.1) (0.3.8)\n",
      "Requirement already satisfied: xxhash in ./miniconda3/lib/python3.8/site-packages (from datasets->trl==0.10.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in ./miniconda3/lib/python3.8/site-packages (from datasets->trl==0.10.1) (0.70.16)\n",
      "Requirement already satisfied: pandas in ./miniconda3/lib/python3.8/site-packages (from datasets->trl==0.10.1) (2.0.3)\n",
      "Requirement already satisfied: aiohttp in ./miniconda3/lib/python3.8/site-packages (from datasets->trl==0.10.1) (3.10.11)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.10.1) (22.1.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.10.1) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.10.1) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.10.1) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.10.1) (1.15.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.10.1) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.10.1) (6.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers==4.45.0) (2.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers==4.45.0) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers==4.45.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/lib/python3.8/site-packages (from requests->transformers==4.45.0) (2021.5.30)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl==0.10.1) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./miniconda3/lib/python3.8/site-packages (from jinja2->torch>=1.4.0->trl==0.10.1) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets->trl==0.10.1) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets->trl==0.10.1) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./miniconda3/lib/python3.8/site-packages (from pandas->datasets->trl==0.10.1) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.10.1) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./miniconda3/lib/python3.8/site-packages (from sympy->torch>=1.4.0->trl==0.10.1) (1.3.0)\n",
      "Installing collected packages: transformers, trl\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.3\n",
      "    Uninstalling transformers-4.46.3:\n",
      "      Successfully uninstalled transformers-4.46.3\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.11.4\n",
      "    Uninstalling trl-0.11.4:\n",
      "      Successfully uninstalled trl-0.11.4\n",
      "Successfully installed transformers-4.45.0 trl-0.10.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install trl==0.10.1 transformers==4.45.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901c7872-b1e4-4d45-a3ca-a940b0822e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.1\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "print(trl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6280aad-a7f3-43da-ad32-092beedd4dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory. Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "# —— 一定要在 import wandb/Trainer 之前就设好环境变量 ——\n",
    "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "# （可选）显式关闭上传延迟，减少初始化等待\n",
    "os.environ[\"WANDB_INIT_TIMEOUT\"] = \"60\"\n",
    "\n",
    "import wandb\n",
    "run = wandb.init(\n",
    "    project=\"my_project\",\n",
    "    name=\"offline-run\",\n",
    "    mode=\"offline\",       # 离线模式\n",
    "    settings=wandb.Settings(init_timeout=60)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ddb2182-a421-4816-815e-92c3b970276f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7950f1d7403348b5a895b8977acc8fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da8663f1b904541afa5509d98b5b6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:23, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.959600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.530600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.349000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.001800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.797600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.968600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.461100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.451700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.0821972465515137, metrics={'train_runtime': 23.9154, 'train_samples_per_second': 4.181, 'train_steps_per_second': 4.181, 'total_flos': 451553771520000.0, 'train_loss': 1.0821972465515137, 'epoch': 10.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "items = []\n",
    "with open(\"./data/sft_data.json\", \"r\", encoding=\"utf8\")as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        items.append({\"prompt\": item[\"query\"], \"completion\": item[\"answer\"]})\n",
    "dataset = Dataset.from_list(items)\n",
    "\n",
    "model_path = 'autodl-tmp/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config,torch_dtype=torch.float16)\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\",\n",
    "                    \"v_proj\",\n",
    "                    \"k_proj\",\n",
    "                    \"o_proj\",\n",
    "                    \"gate_proj\",\n",
    "                    \"down_proj\",\n",
    "                    \"up_proj\"\n",
    "                    ],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "sft_config = SFTConfig(output_dir=\"/tmp\",\n",
    "                       neftune_noise_alpha=10, # noise\n",
    "                       per_device_train_batch_size=1,\n",
    "                       max_seq_length=100,\n",
    "                       num_train_epochs=10,\n",
    "                       logging_steps=10,\n",
    "                       logging_strategy=\"steps\")\n",
    "response_template = \"<|start_header_id|>assistant<|end_header_id\"\"|>\\n\\n\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config,\n",
    "    data_collator=collator\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "199139fc",
   "metadata": {
    "id": "199139fc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2229945511b451ca65dcbabeb361fa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at autodl-tmp/Llama-3.1-8B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,975,616 || all params: 7,525,904,384 || trainable%: 0.2787\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09bb31058a04338ac990f394a0260d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
      "    num_rows: 10\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/root/miniconda3/lib/python3.8/site-packages/trl/trainer/reward_trainer.py:176: UserWarning: When using RewardDataCollatorWithPadding, you should set `max_length` in RewardConfig. It will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/trl/trainer/reward_trainer.py:193: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2855: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.397000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.294100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.151200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.675500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "model_path = r'autodl-tmp/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path,\n",
    "                                                           num_labels=1,\n",
    "                                                           quantization_config=bnb_config)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\",\n",
    "                    \"v_proj\",\n",
    "                    \"k_proj\",\n",
    "                    \"o_proj\",\n",
    "                    \"gate_proj\",\n",
    "                    \"down_proj\",\n",
    "                    \"up_proj\"\n",
    "                    ],\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "items = []\n",
    "with open(\"data_ppo/preference.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        items.append(item)\n",
    "\n",
    "dataset = Dataset.from_list(items)\n",
    "\n",
    "\n",
    "def process_func(example):\n",
    "    chosen = example[\"question\"] + example[\"chosen\"]\n",
    "    rejected = example[\"question\"] + example[\"rejected\"]\n",
    "\n",
    "    tokenized_chosen = tokenizer(chosen)\n",
    "    tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "    new_example = {}\n",
    "    new_example[\"input_ids_chosen\"] = tokenized_chosen[\"input_ids\"]\n",
    "    new_example[\"attention_mask_chosen\"] = tokenized_chosen[\"attention_mask\"]\n",
    "    new_example[\"input_ids_rejected\"] = tokenized_rejected[\"input_ids\"]\n",
    "    new_example[\"attention_mask_rejected\"] = tokenized_rejected[\"attention_mask\"]\n",
    "    return new_example\n",
    "\n",
    "\n",
    "dataset = dataset.map(process_func, remove_columns=['question', 'chosen', 'rejected'])\n",
    "print(dataset)\n",
    "\n",
    "config = RewardConfig(output_dir=\"./reward_model\",\n",
    "                      logging_strategy=\"steps\",\n",
    "                      logging_steps=1,\n",
    "                      logging_first_step=True)\n",
    "config.num_train_epochs = 1\n",
    "config.per_device_train_batch_size = 1\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=config,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"./reward_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "963e47e3-ebbc-42a9-acf0-2b6ac4bfcf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658db68bca75463c80a5dee52b8c8e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from 'autodl-tmp/Llama-3.1-8B-Instruct', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "/root/miniconda3/lib/python3.8/site-packages/trl/trainer/ppo_trainer.py:1423: UserWarning: Cannot retrieve user information assuming you are running in offline mode.\n",
      "  warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "model_path = 'autodl-tmp/Llama-3.1-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\",\n",
    "                    \"v_proj\",\n",
    "                    \"k_proj\",\n",
    "                    \"o_proj\",\n",
    "                    \"gate_proj\",\n",
    "                    \"down_proj\",\n",
    "                    \"up_proj\"\n",
    "                    ],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_path,\n",
    "                                                          reward_adapter=\"./reward_model\",\n",
    "                                                          peft_config=peft_config,\n",
    "                                                          quantization_config=bnb_config\n",
    "                                                          )\n",
    "model.to(\"cuda\")\n",
    "\n",
    "items = []\n",
    "with open(\"./data_ppo/queries.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        items.append(json.loads(line))\n",
    "queries_dataset = Dataset.from_list(items)\n",
    "\n",
    "\n",
    "def collator(data):\n",
    "    queries = []\n",
    "    for item in data:\n",
    "        queries.append(tokenizer(item[\"query\"], return_tensors=\"pt\")[\"input_ids\"].squeeze().to(\"cuda\"))\n",
    "    return queries\n",
    "\n",
    "\n",
    "ppo_config = PPOConfig(kl_penalty=\"full\", ppo_epochs=3, batch_size=2, mini_batch_size=1)\n",
    "ppo_trainer = PPOTrainer(config=ppo_config, model=model, ref_model=None, tokenizer=tokenizer, dataset=queries_dataset,\n",
    "                         data_collator=collator)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    \"max_new_tokens\": 32,\n",
    "}\n",
    "\n",
    "for batch in ppo_trainer.dataloader:\n",
    "    query_tensors = batch\n",
    "\n",
    "    response_tensors = ppo_trainer.generate(\n",
    "        query_tensors, return_prompt=False,  **generation_kwargs)\n",
    "    scores = []\n",
    "    for query, response in zip(query_tensors, response_tensors):\n",
    "        input_ids = torch.concat([query, response], dim=0)\n",
    "        input_ids = torch.unsqueeze(input_ids, dim=0)\n",
    "        score = ppo_trainer.model.compute_reward_score(input_ids=input_ids)[0, -1, 0]\n",
    "        scores.append(score)\n",
    "    stats = ppo_trainer.step(query_tensors, response_tensors, scores)\n",
    "ppo_trainer.save_pretrained(\"./rl_model\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "016c126a7ccc4770a74d5e4356600fac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01bf358143c94430bf6c5ef0f43d70cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bb7cb9a35fd48529e8f2148f32ca081": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d029aa4596f4bb0a91f0a979dd858d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01bf358143c94430bf6c5ef0f43d70cc",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a33c8575a17541209d30ed0203302a91",
      "value": 0
     }
    },
    "4d3a09097b1b42efb43277f73173cd8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8afef6604014bef8a34fe312783f201",
       "IPY_MODEL_3d029aa4596f4bb0a91f0a979dd858d9",
       "IPY_MODEL_e6513eb0700f442ba85e13fc3ad87621"
      ],
      "layout": "IPY_MODEL_1bb7cb9a35fd48529e8f2148f32ca081"
     }
    },
    "58b727b75e694a329667b34aae3676b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a33c8575a17541209d30ed0203302a91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c73842eb644b44b58007a1ea09c9ca67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8afef6604014bef8a34fe312783f201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_016c126a7ccc4770a74d5e4356600fac",
      "placeholder": "​",
      "style": "IPY_MODEL_58b727b75e694a329667b34aae3676b3",
      "value": "Loading checkpoint shards:   0%"
     }
    },
    "d81f3087f9f844bfb6263302eee0acaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e6513eb0700f442ba85e13fc3ad87621": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c73842eb644b44b58007a1ea09c9ca67",
      "placeholder": "​",
      "style": "IPY_MODEL_d81f3087f9f844bfb6263302eee0acaf",
      "value": " 0/4 [00:00&lt;?, ?it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
